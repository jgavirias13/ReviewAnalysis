{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraccion de informacion\n",
    "\n",
    "Script para extraer información desde Metacritic por medio de tecnicas de web scraping. Deja como salida un archivo csv con la información de las reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random as rand \n",
    "import pandas as pd\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict = pd.read_csv('./trainingSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura del archivo de urls\n",
    "resources = pd.read_csv('./urls.csv')\n",
    "urls = resources.loc[resources['status'] != 'S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura del archivo de control\n",
    "control = pd.read_csv('./control.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuracion de request\n",
    "headers = {'User-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Procesando url 1 de 1\n  14.3%  28.6%  42.9%  57.1%  71.4%  85.7%  100.0%"
    }
   ],
   "source": [
    "i = 1\n",
    "for url in urls.url:\n",
    "    print('Procesando url {} de {}'.format(i, len(urls)))\n",
    "    # Se toma el numero de paginas de reviews\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if(response.status_code != 200):\n",
    "        resources.loc[resources.url == url, 'status'] = 'F'\n",
    "        continue\n",
    "    resources.loc[resources.url == url, 'status'] = 'W'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    lastPageList = soup.find('li', class_='last_page')\n",
    "    numPages = 10\n",
    "    if(lastPageList):\n",
    "        pageRef = lastPageList.find('a', class_='page_num')\n",
    "        if(pageRef):\n",
    "            numPages = int(pageRef.text)\n",
    "    resources.loc[resources.url == url, 'paginasProcesadas'] = 0\n",
    "    resources.loc[resources.url == url, 'paginasTotales'] = numPages\n",
    "    for page in range(0,numPages):\n",
    "        print('  {:.1%}\\r'.format((page+1)/numPages),end='')\n",
    "        filter1 = control['url'] == url\n",
    "        filter2 = control['page'] == page\n",
    "        filter3 = control['status'] == 'S'\n",
    "        register = control.query('url == \"'+url+'\" and page == '+str(page)+' and status == \"S\"')\n",
    "        if(len(register) > 0):\n",
    "            resources.loc[resources.url == url, 'paginasProcesadas'] += 1\n",
    "            continue\n",
    "        headers = {'User-agent': 'Mozilla/5.0'}\n",
    "        reviewUrl = url + '?page=' + str(numPages)\n",
    "        response  = requests.get(url, headers = headers)\n",
    "        if(response.status_code != 200):\n",
    "            if(len(control.loc[filter1 & filter2]) > 0):\n",
    "                control.loc[filter1 & filter2,'status'] = 'F'\n",
    "            else:\n",
    "                control.loc[len(control)] = {'url':url,'page':page,'status':'F'}\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.find('div', class_='product_title')\n",
    "        reviews = soup.find('div', class_='product_reviews')\n",
    "        product = ''\n",
    "        platform = ''\n",
    "        if(title):\n",
    "            productHead = title.find('h1')\n",
    "            if(productHead):\n",
    "                product = productHead.text\n",
    "            platformHead = title.find('span', class_='platform')\n",
    "            if(platformHead):\n",
    "                platform = platformHead.text.strip()\n",
    "        if(product):\n",
    "            for review in reviews.find_all('div', class_='review_content'):\n",
    "                row = {'name':'', 'product':'', 'platform':'', 'date':'', 'rating':'', 'upVotes':'', 'totVotes':'', 'review':'', 'langreview':''}\n",
    "                name = review.find('div', class_='name')\n",
    "                if name:\n",
    "                    row['name'] = name.text.strip().replace('\\n','')\n",
    "                row['product'] = product\n",
    "                row['platform'] = platform\n",
    "                date = review.find('div', class_='date')\n",
    "                if(date):\n",
    "                   row['date'] = date.text \n",
    "                rating = review.find('div', class_='review_grade')\n",
    "                if(rating):\n",
    "                    row['rating'] = rating.text.strip().replace('\\n','')\n",
    "                upsVotes = review.find('span', class_='total_ups')\n",
    "                if(upsVotes):\n",
    "                    row['upVotes'] = upsVotes.text\n",
    "                totVotes = review.find('span', class_='total_thumbs')\n",
    "                if(totVotes):\n",
    "                    row['totVotes'] = totVotes.text\n",
    "                content = review.find('div', class_='review_body')\n",
    "                if(content):\n",
    "                    row['review'] = content.text.replace('\\n',' ').strip()\n",
    "                else:\n",
    "                    content = review.find('span', class_='blurb blurb_expanded')\n",
    "                    if(content):\n",
    "                        row['review'] = content.text.replace('\\n',' ').strip()\n",
    "                review_dict.loc[len(review_dict)] = row\n",
    "            if(len(control.loc[filter1 & filter2]) > 0):\n",
    "                control.loc[filter1 & filter2,'status'] = 'S'\n",
    "            else:\n",
    "                control.loc[len(control)] = {'url':url,'page':page,'status':'S'}\n",
    "            resources.loc[resources.url == url, 'paginasProcesadas'] += 1\n",
    "    if(resources.loc[resources.url == url, 'paginasProcesadas'][0] == resources.loc[resources.url == url, 'paginasTotales'][0]):\n",
    "        resources.loc[resources.url == url, 'status'] = 'S'\n",
    "    else:\n",
    "        resources.loc[resources.url == url, 'status'] = 'P'\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict['langreview'] = review_dict['review'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "control.to_csv('control.csv',index=False)\n",
    "resources.to_csv('urls.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict.to_csv('trainingSet.csv', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}